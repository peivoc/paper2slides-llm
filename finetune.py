import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig
from trl import SFTTrainer
import os

# --- 1. è¨­å®šæ¨¡å‹å’Œè³‡æ–™é›†è·¯å¾‘ ---
# æˆ‘å€‘é¸æ“‡ Mistral-7B ä½œç‚ºåŸºç¤æ¨¡å‹ã€‚å®ƒåœ¨æ€§èƒ½å’Œè³‡æºéœ€æ±‚ä¹‹é–“å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚
# Gemma-2B æ˜¯å¦ä¸€å€‹å¾ˆå¥½çš„é¸æ“‡ï¼Œå¦‚æœ Mistral-7B å°æ‚¨çš„ç¡¬é«”ä¾†èªªå¤ªå¤§çš„è©±ã€‚
model_name = "google/gemma-2b-it"
dataset_path = "data/finetuning/training_dataset.jsonl" # æˆ‘å€‘å‰›å‰›å»ºç«‹çš„è³‡æ–™é›†æª”æ¡ˆ
output_dir = "results/finetuned_adapter" # è¨“ç·´å®Œæˆå¾Œï¼Œå„²å­˜æ¨¡å‹ adapter çš„åœ°æ–¹

# --- 2. è¨­å®š QLoRA é‡åŒ– ---
# é€™æ˜¯èƒ½åœ¨ 4GB VRAM ä¸Šé€²è¡Œå¾®èª¿çš„é—œéµï¼
# æˆ‘å€‘å°‡æ¨¡å‹æ¬Šé‡ä»¥ 4-bit çš„ç²¾åº¦è¼‰å…¥ï¼Œå¤§å¤§æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨ã€‚
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16, # ä½¿ç”¨ bfloat16 ä¾†é€²è¡Œè¨ˆç®—ï¼Œä»¥ä¿æŒç²¾åº¦
    bnb_4bit_use_double_quant=False,
)

# --- 3. è¼‰å…¥æ¨¡å‹å’Œ Tokenizer ---
print("è¼‰å…¥æ¨¡å‹å’Œ Tokenizer...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto" # è‡ªå‹•å°‡æ¨¡å‹åˆ†é…åˆ°å¯ç”¨çš„ç¡¬é«”ä¸Šï¼ˆä¾‹å¦‚ GPUï¼‰
)
model.config.use_cache = False # åœ¨è¨“ç·´æ™‚é—œé–‰å¿«å–ï¼Œé€™æ˜¯ä¸€å€‹æ¨è–¦çš„æœ€ä½³å¯¦è¸


tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# è¨­å®š padding tokenã€‚å¦‚æœ tokenizer æ²’æœ‰ pad_tokenï¼Œæˆ‘å€‘é€šå¸¸æœƒå°‡å…¶è¨­å®šç‚º eos_tokenã€‚
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # å°‡ padding æ”¾åœ¨å³å´ï¼Œä»¥é¿å… T5 é€™é¡æ¨¡å‹çš„å•é¡Œ

# --- 4. è¨­å®š PEFT (LoRA) ---
# æˆ‘å€‘åªè¨“ç·´ä¸€å°éƒ¨åˆ†çš„ "adapter" æ¬Šé‡ï¼Œè€Œä¸æ˜¯æ•´å€‹æ¨¡å‹ã€‚
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

# --- 5. è¨­å®šè¨“ç·´åƒæ•¸ ---
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=1,                # è¨“ç·´çš„ç¸½è¼ªæ•¸ã€‚å…ˆå¾ 1 é–‹å§‹ï¼Œä¹‹å¾Œå¯ä»¥å¢åŠ ã€‚
    per_device_train_batch_size=1,     # ç”±æ–¼ VRAM æœ‰é™ï¼Œæˆ‘å€‘ä¸€æ¬¡åªè™•ç†ä¸€å€‹æ¨£æœ¬
    gradient_accumulation_steps=4,     # æ¢¯åº¦ç´¯ç©ã€‚æ•ˆæœç­‰åŒæ–¼ batch_size = 1 * 4 = 4
    optim="paged_adamw_32bit",         # ä½¿ç”¨åˆ†é å„ªåŒ–å™¨ä»¥ç¯€çœè¨˜æ†¶é«”
    save_steps=25,                     # æ¯ 25 å€‹æ­¥é©Ÿå„²å­˜ä¸€æ¬¡ checkpoint
    logging_steps=5,                   # æ¯ 5 å€‹æ­¥é©Ÿè¨˜éŒ„ä¸€æ¬¡ log
    learning_rate=2e-4,                # å­¸ç¿’ç‡
    weight_decay=0.001,
    fp16=False,                        # æˆ‘å€‘ä½¿ç”¨ bfloat16ï¼Œæ‰€ä»¥é—œé–‰ fp16
    bf16=True,                         # å•Ÿç”¨ bfloat16 è¨“ç·´
    max_grad_norm=0.3,
    max_steps=-1,                      # å¦‚æœè¨­å®šäº†ï¼Œæœƒè¦†å¯« num_train_epochs
    warmup_ratio=0.03,
    group_by_length=True,              # å°‡é•·åº¦ç›¸è¿‘çš„æ¨£æœ¬åˆ†çµ„ï¼Œä»¥æé«˜æ•ˆç‡
    lr_scheduler_type="constant",      # å­¸ç¿’ç‡æ’ç¨‹å™¨
)

# --- 6. å»ºç«‹ä¸¦é–‹å§‹è¨“ç·´ ---
print("è¼‰å…¥è³‡æ–™é›†...")
# æˆ‘å€‘éœ€è¦å…ˆè¼‰å…¥è³‡æ–™é›†
dataset = load_dataset("json", data_files=dataset_path, split="train")

print("è¨­å®š SFT Trainer...")
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text", # å‘Šè¨´ trainer æˆ‘å€‘è³‡æ–™é›†è£¡çš„æ–‡å­—æ¬„ä½å«åš "text"
    max_seq_length=2048,       # åºåˆ—çš„æœ€å¤§é•·åº¦ã€‚å¯ä»¥æ ¹æ“šæ‚¨çš„ VRAM å’Œè³‡æ–™é€²è¡Œèª¿æ•´ã€‚
    tokenizer=tokenizer,
    args=training_arguments,
    packing=False,             # æ˜¯å¦å°‡å¤šå€‹çŸ­æ¨£æœ¬æ‰“åŒ…æˆä¸€å€‹é•·æ¨£æœ¬
)

print("ğŸš€ é–‹å§‹å¾®èª¿ï¼")
trainer.train()

# --- 7. å„²å­˜æœ€çµ‚çš„æ¨¡å‹ adapter ---
print("âœ… å¾®èª¿å®Œæˆï¼Œæ­£åœ¨å„²å­˜ adapter...")
trainer.save_model(output_dir)
print(f"ğŸ‰ æˆåŠŸï¼Adapter å·²å„²å­˜è‡³ {output_dir}")
