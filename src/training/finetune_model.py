import os
import torch
from datasets import load_dataset
from transformers (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer
from pathlib import Path

# å®šç¾©è·¯å¾‘
TRAINING_DATA_PATH = Path("data/training/training_data.jsonl")
MODEL_OUTPUT_DIR = Path("models/gemma-2b-finetuned")

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def finetune_gemma_2b():
    print("\nğŸš€ é–‹å§‹ Gemma-2B æ¨¡å‹å¾®èª¿æµç¨‹...")

    # 1. è¼‰å…¥è¨“ç·´è³‡æ–™é›†
    if not TRAINING_DATA_PATH.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™é›† {TRAINING_DATA_PATH}ã€‚è«‹ç¢ºä¿æ‚¨å·²åŸ·è¡Œ prepare_training_data.py ä¸¦ç”Ÿæˆäº†è³‡æ–™ã€‚")
        return
    
    # datasets å‡½å¼åº«å¯ä»¥å¾ jsonl æª”æ¡ˆè¼‰å…¥è³‡æ–™
    dataset = load_dataset("json", data_files=str(TRAINING_DATA_PATH), split="train")
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(dataset)} å€‹è¨“ç·´ç¯„ä¾‹ã€‚")
    # print("è³‡æ–™é›†ç¯„ä¾‹:", dataset[0]) # å¯ä»¥å–æ¶ˆè¨»è§£æŸ¥çœ‹è³‡æ–™é›†çµæ§‹

    # 2. è¨­å®šåŸºç¤æ¨¡å‹å’Œåˆ†è©å™¨
    model_name = "google/gemma-2b"
    
    # 4-bit é‡åŒ–è¨­å®šï¼Œç”¨æ–¼ç¯€çœ GPU è¨˜æ†¶é«”
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=False,
    )

    # è¼‰å…¥åŸºç¤æ¨¡å‹ (Gemma-2B)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    model.config.use_cache = False
    model.config.pretraining_tp = 1

    # è¼‰å…¥åˆ†è©å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right" # Gemma æ¨¡å‹çš„å»ºè­°è¨­å®š

    print("âœ… åŸºç¤æ¨¡å‹å’Œåˆ†è©å™¨è¼‰å…¥å®Œæˆã€‚")

    # 3. è¨­å®š LoRA (Parameter-Efficient Fine-Tuning)
    peft_config = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        bias="none",
        task_type="CAUSAL_LM",
    )
    print("âœ… LoRA è¨­å®šå®Œæˆã€‚")

    # 4. è¨­å®šè¨“ç·´åƒæ•¸
    training_arguments = TrainingArguments(
        output_dir=str(MODEL_OUTPUT_DIR),
        num_train_epochs=1,  # ç‚ºäº†ç·´ç¿’ï¼Œå…ˆè¨­å®šç‚º 1 å€‹ epoch
        per_device_train_batch_size=2, # æ ¹æ“š Colab GPU è¨˜æ†¶é«”èª¿æ•´
        gradient_accumulation_steps=1, # æ¢¯åº¦ç´¯ç©æ­¥æ•¸
        optim="paged_adamw_8bit", # 8-bit AdamW å„ªåŒ–å™¨
        save_steps=0, # ä¸åœ¨è¨“ç·´éç¨‹ä¸­å„²å­˜æª¢æŸ¥é»
        logging_steps=25, # æ¯ 25 æ­¥è¨˜éŒ„ä¸€æ¬¡æ—¥èªŒ
        learning_rate=2e-4,
        weight_decay=0.001,
        fp16=False, # Gemma å»ºè­°ä½¿ç”¨ bfloat16
        bf16=True,
        max_grad_norm=0.3,
        max_steps=-1, # æ ¹æ“š num_train_epochs æ±ºå®šæ­¥æ•¸
        warmup_ratio=0.03,
        group_by_length=True,
        lr_scheduler_type="cosine",
        report_to="none" # ä¸ä¸Šå ±åˆ°ä»»ä½•å¹³å°ï¼Œä¾‹å¦‚ wandb
    )
    print("âœ… è¨“ç·´åƒæ•¸è¨­å®šå®Œæˆã€‚")

    # 5. åˆå§‹åŒ– SFTTrainer
    # æˆ‘å€‘éœ€è¦ä¸€å€‹æ ¼å¼åŒ–å‡½å¼ä¾†å°‡ dataset ä¸­çš„ prompt å’Œ completion çµ„åˆèµ·ä¾†
    def formatting_prompts_func(examples):
        output_texts = []
        for i in range(len(examples["prompt"])):
            # é€™è£¡çš„æ ¼å¼éœ€è¦èˆ‡æ‚¨åœ¨ prompt_generator.py ä¸­çµ¦æ¨¡å‹çš„æŒ‡ä»¤æ ¼å¼ä¸€è‡´
            # ä¸¦ä¸”å°‡ completion æ”¾åœ¨ prompt ä¹‹å¾Œï¼Œä½œç‚ºæ¨¡å‹å­¸ç¿’çš„ç›®æ¨™
            text = examples["prompt"][i] + examples["completion"][i]
            output_texts.append(text)
        return output_texts

    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        peft_config=peft_config,
        tokenizer=tokenizer,
        formatting_func=formatting_prompts_func,
        args=training_arguments,
        max_seq_length=1024, # æ ¹æ“šæ‚¨çš„è³‡æ–™é•·åº¦èª¿æ•´ï¼Œéé•·æœƒå°è‡´ OOM
    )
    print("âœ… SFTTrainer åˆå§‹åŒ–å®Œæˆã€‚")

    # 6. é–‹å§‹è¨“ç·´
    print("\nğŸ”¥ é–‹å§‹è¨“ç·´æ¨¡å‹...é€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ã€‚")
    trainer.train()
    print("\nğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆï¼")

    # 7. å„²å­˜å¾®èª¿å¾Œçš„æ¨¡å‹
    trainer.save_model(MODEL_OUTPUT_DIR)
    print(f"âœ… å¾®èª¿å¾Œçš„æ¨¡å‹å·²å„²å­˜è‡³: {MODEL_OUTPUT_DIR}")

    # æ¸…ç† GPU è¨˜æ†¶é«”
    del model
    del tokenizer
    del trainer
    torch.cuda.empty_cache()

if __name__ == "__main__":
    finetune_gemma_2b()
