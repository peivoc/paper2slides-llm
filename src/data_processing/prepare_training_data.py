import json
from pathlib import Path
import re
import sys

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„æ·»åŠ åˆ° Python çš„æœç´¢è·¯å¾‘ä¸­
# é€™æ¨£å³ä½¿å¾å­ç›®éŒ„åŸ·è¡Œï¼Œä¹Ÿèƒ½æ­£ç¢ºå°å…¥æ¨¡çµ„
project_root = Path(__file__).resolve().parents[2]
sys.path.append(str(project_root))

from src.data_processing.prompt_generator import convert_to_prompt

# å®šç¾©è³‡æ–™å¤¾è·¯å¾‘
PROCESSED_DIR = Path("data/processed")
SLIDES_DIR = Path("data/slides")
TRAINING_DIR = Path("data/training")

TRAINING_DIR.mkdir(parents=True, exist_ok=True)

def prepare_training_dataset(output_filename: str = "training_data.jsonl"):
    """
    æº–å‚™ç”¨æ–¼æ¨¡å‹å¾®èª¿çš„è¨“ç·´è³‡æ–™é›†ã€‚
    å®ƒæœƒå°‡è™•ç†éçš„è«–æ–‡è³‡æ–™ (Prompt) èˆ‡äººå·¥ä¿®æ”¹å¾Œçš„ç°¡å ± (Completion) é…å°ã€‚
    """
    print("\nğŸš€ é–‹å§‹æº–å‚™è¨“ç·´è³‡æ–™é›†...")
    
    training_examples = []
    
    # éæ­·æ‰€æœ‰å·²è™•ç†çš„è«–æ–‡ JSON æª”æ¡ˆ
    processed_files = list(PROCESSED_DIR.glob("*.json"))
    # æ’é™¤ processing_summary.json
    processed_files = [f for f in processed_files if f.name != 'processing_summary.json']

    if not processed_files:
        print("â— åœ¨ data/processed/ ä¸­æ‰¾ä¸åˆ°ä»»ä½•å·²è™•ç†çš„è«–æ–‡ JSON æª”æ¡ˆã€‚è«‹å…ˆåŸ·è¡Œ main.py è™•ç†è«–æ–‡ã€‚")
        return

    for processed_file in processed_files:
        try:
            with open(processed_file, 'r', encoding='utf-8') as f:
                processed_data = json.load(f)
            
            # å¾è™•ç†éçš„æª”æ¡ˆåä¸­æå–åŸºç¤åç¨± (ä¾‹å¦‚ '2104.05740v1' from '2104.05740v1.json')
            # æˆ–è€…å¦‚æœæˆ‘å€‘æ¡ç”¨äº†æˆªæ–·é‚è¼¯ï¼Œå°±æ˜¯ '2104' from '2104.json'
            # é€™è£¡æˆ‘å€‘ä½¿ç”¨ processed_file.stemï¼Œå®ƒæœƒæ˜¯å®Œæ•´çš„æª”åï¼ˆä¸å«.jsonï¼‰
            # ç‚ºäº†å…¼å®¹ä¹‹å‰çš„æª”åæˆªæ–·é‚è¼¯ï¼Œæˆ‘å€‘éœ€è¦æ›´éˆæ´»åœ°åŒ¹é…
            base_name = processed_file.stem # ä¾‹å¦‚ '2104.05740v1' æˆ– '2104'

            # å°‹æ‰¾å°æ‡‰çš„ç°¡å ± Markdown æª”æ¡ˆ
            # ç°¡å ±æª”æ¡ˆåå¯èƒ½åŒ…å«æ™‚é–“æˆ³ï¼Œæ‰€ä»¥æˆ‘å€‘ç”¨ glob æ¨¡å¼åŒ¹é…é–‹é ­
            # é€™è£¡å‡è¨­äººå·¥ä¿®æ”¹å¾Œçš„ç°¡å ±æª”æ¡ˆåä»ç„¶ä»¥åŸå§‹è«–æ–‡çš„ base_name é–‹é ­
            # ä¾‹å¦‚ï¼š2104.05740v1_slides_20240717_123456.md æˆ– 2104_my_edited_slides.md
            slide_files = list(SLIDES_DIR.glob(f"{base_name}*.md"))
            
            if not slide_files:
                print(f"âš ï¸ æ‰¾ä¸åˆ°èˆ‡ {processed_file.name} å°æ‡‰çš„ç°¡å ± Markdown æª”æ¡ˆã€‚è«‹ç¢ºä¿æ‚¨å·²ä¿®æ”¹ä¸¦å„²å­˜äº†ç°¡å ±ã€‚")
                continue
            
            # å¦‚æœæœ‰å¤šå€‹åŒ¹é…çš„ç°¡å ±æª”æ¡ˆï¼Œæˆ‘å€‘é¸æ“‡æœ€æ–°çš„é‚£ä¸€å€‹
            slide_file = max(slide_files, key=lambda f: f.stat().st_mtime)
            
            with open(slide_file, 'r', encoding='utf-8') as f:
                completion_text = f.read()
            
            # ç”Ÿæˆ Prompt
            prompt_text = convert_to_prompt(processed_data)
            
            # å°‡ Prompt å’Œ Completion åˆä½µç‚º SFTTrainer éœ€è¦çš„æ ¼å¼
            # æ ¼å¼ç‚º: {"text": "<s>[INST] promptå…§å®¹ [/INST] completionå…§å®¹ </s>"}
            formatted_text = f"<s>[INST] {prompt_text} [/INST] {completion_text} </s>"
            training_examples.append({"text": formatted_text})
            print(f"âœ… å·²é…å°ä¸¦è½‰æ› {processed_file.name} å’Œ {slide_file.name}")

        except Exception as e:
            print(f"âŒ è™•ç† {processed_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue

    if not training_examples:
        print("âŒ æœªç”Ÿæˆä»»ä½•è¨“ç·´ç¯„ä¾‹ã€‚è«‹æª¢æŸ¥ data/processed/ å’Œ data/slides/ ä¸­çš„æª”æ¡ˆã€‚")
        return

    # å°‡è¨“ç·´ç¯„ä¾‹å¯«å…¥ JSONL æª”æ¡ˆ
    output_path = TRAINING_DIR / output_filename
    with open(output_path, 'w', encoding='utf-8') as f:
        for example in training_examples:
            f.write(json.dumps(example, ensure_ascii=False) + '\n')
    
    print(f"\nğŸ‰ è¨“ç·´è³‡æ–™é›†å·²æˆåŠŸç”Ÿæˆè‡³: {output_path}")
    print(f"   å…±ç”Ÿæˆ {len(training_examples)} å€‹è¨“ç·´ç¯„ä¾‹ã€‚")

if __name__ == "__main__":
    prepare_training_dataset()
