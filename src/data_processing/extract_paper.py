import os
import re
from pathlib import Path
from pypdf import PdfReader
import json
from datetime import datetime

RAW_DIR = Path("data/raw")
OUTPUT_DIR = Path("data/processed")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def extract_text_from_pdf(pdf_path):
    """å¾PDFæå–æ–‡æœ¬ï¼Œä¸¦è™•ç†å¸¸è¦‹çš„PDFå•é¡Œ"""
    try:
        reader = PdfReader(str(pdf_path))
        text = ""
        total_pages = len(reader.pages)
        
        for i, page in enumerate(reader.pages):
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
            
            # é€²åº¦é¡¯ç¤º
            if i % 10 == 0:
                print(f"   è™•ç†é€²åº¦: {i+1}/{total_pages} é ")
        
        return text
    except Exception as e:
        print(f"âŒ PDFè®€å–å¤±æ•—: {e}")
        return ""


def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤PDFå¸¸è¦‹çš„æ ¼å¼å•é¡Œ"""
    if not text:
        return ""
    
    # ç§»é™¤é ç¢¼å’Œé çœ‰é è…³ï¼ˆå¸¸è¦‹æ¨¡å¼ï¼‰
    text = re.sub(r'\n\s*\d+\s*\n', '\n', text)  # å–®ç¨çš„æ•¸å­—è¡Œ
    text = re.sub(r'\n\s*Page \d+.*?\n', '\n', text)  # Page X of Y
    
    # è™•ç†é€£å­—ç¬¦è™Ÿæ–·è¡Œ
    text = re.sub(r'([a-z])-\n([a-z])', r'\1\2', text)
    
    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œæ›è¡Œ
    text = re.sub(r'\n\s*\n', '\n\n', text)  # å¤šå€‹ç©ºè¡Œè®Šæˆå…©å€‹
    text = re.sub(r'[ \t]+', ' ', text)  # å¤šå€‹ç©ºæ ¼è®Šæˆä¸€å€‹
    
    # ç§»é™¤é¦–å°¾ç©ºç™½
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(lines)


def extract_paper_metadata(text):
    """å˜—è©¦æå–è«–æ–‡åŸºæœ¬è³‡è¨Š"""
    metadata = {}
    
    # æå–æ¨™é¡Œï¼ˆé€šå¸¸åœ¨å‰å¹¾è¡Œï¼‰
    lines = text.split('\n')[:20]  # åªçœ‹å‰20è¡Œ
    title_candidates = []
    
    for line in lines:
        if len(line) > 20 and len(line) < 200:  # æ¨™é¡Œé•·åº¦åˆç†
            if not re.search(r'^\d+\.|abstract|introduction|arxiv', line.lower()):
                title_candidates.append(line)
    
    if title_candidates:
        metadata['title'] = title_candidates[0]
    
    # æå–æ‘˜è¦
    abstract_match = re.search(r'abstract\s*[:\-]?\s*(.*?)(?=\n\s*\n|\nintroduction|\n1\.|\nkeywords)', 
                              text, re.IGNORECASE | re.DOTALL)
    if abstract_match:
        metadata['abstract'] = abstract_match.group(1).strip()
    
    # æå–é—œéµè©
    keywords_match = re.search(r'keywords?\s*[:\-]?\s*(.*?)(?=\n\s*\n|\nintroduction|\n1\.)', 
                              text, re.IGNORECASE | re.DOTALL)
    if keywords_match:
        metadata['keywords'] = keywords_match.group(1).strip()
    
    return metadata


def split_into_sections(text):
    """å°‡æ–‡æœ¬åˆ†å‰²ç‚ºç« ç¯€"""
    sections = []
    
    # å¸¸è¦‹çš„ç« ç¯€æ¨™é¡Œæ¨¡å¼
    section_patterns = [
        r'\n\s*(\d+\.?\s+[A-Z][^.\n]*)\n',  # 1. Introduction
        r'\n\s*([A-Z][A-Z\s]{2,})\n',      # ABSTRACT, INTRODUCTION
        r'\n\s*(Abstract|Introduction|Related Work|Methodology|Experiments|Results|Conclusion|References)\s*\n',
    ]
    
    # æ‰¾åˆ°æ‰€æœ‰ç« ç¯€æ¨™é¡Œ
    section_breaks = [(0, "é–‹å§‹")]
    
    for pattern in section_patterns:
        matches = list(re.finditer(pattern, text, re.IGNORECASE))
        for match in matches:
            section_title = match.group(1).strip()
            if len(section_title) < 100:  # é¿å…èª¤åˆ¤
                section_breaks.append((match.start(), section_title))
    
    # æŒ‰ä½ç½®æ’åº
    section_breaks.sort(key=lambda x: x[0])
    
    # æå–ç« ç¯€å…§å®¹
    for i in range(len(section_breaks) - 1):
        start_pos = section_breaks[i][0]
        end_pos = section_breaks[i + 1][0]
        section_title = section_breaks[i][1]
        section_content = text[start_pos:end_pos].strip()
        
        if len(section_content) > 100:  # éçŸ­çš„ç« ç¯€å¯èƒ½æ˜¯èª¤åˆ¤
            sections.append({
                'title': section_title,
                'content': section_content
            })
    
    # è™•ç†æœ€å¾Œä¸€å€‹ç« ç¯€
    if section_breaks:
        last_section = text[section_breaks[-1][0]:].strip()
        if len(last_section) > 100:
            sections.append({
                'title': section_breaks[-1][1],
                'content': last_section
            })
    
    return sections


def split_into_paragraphs(text, min_length=50):
    """å°‡æ–‡æœ¬åˆ†å‰²ç‚ºæ®µè½ï¼Œæ”¹é€²ç‰ˆæœ¬"""
    # å…ˆæŒ‰é›™æ›è¡Œåˆ†å‰²
    paragraphs = text.split('\n\n')
    
    # é€²ä¸€æ­¥è™•ç†
    processed_paragraphs = []
    for para in paragraphs:
        para = para.strip()
        
        # éæ¿¾æ¢ä»¶
        if len(para) < min_length:
            continue
        
        # è·³éå¯èƒ½çš„é ç¢¼ã€é çœ‰é è…³
        if re.match(r'^\d+$', para) or re.match(r'^Page \d+', para):
            continue
        
        # è·³éç´”æ•¸å­—æˆ–ç¬¦è™Ÿ
        if re.match(r'^[\d\s\-\.\(\)]+$', para):
            continue
        
        processed_paragraphs.append(para)
    
    return processed_paragraphs


def save_processed_data(data, output_path):
    """å„²å­˜è™•ç†å¾Œçš„è³‡æ–™ï¼Œæ”¯æ´å¤šç¨®æ ¼å¼"""
    # å„²å­˜ç‚ºJSONæ ¼å¼ï¼ˆçµæ§‹åŒ–è³‡æ–™ï¼‰
    json_path = output_path.with_suffix('.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # å„²å­˜ç‚ºç´”æ–‡æœ¬æ ¼å¼ï¼ˆæ˜“è®€ï¼‰
    txt_path = output_path.with_suffix('.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(f"è«–æ–‡è™•ç†çµæœ\n")
        f.write(f"è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 50 + "\n\n")
        
        # å¯«å…¥å…ƒè³‡æ–™
        if 'metadata' in data:
            f.write("ğŸ“‹ è«–æ–‡è³‡è¨Š\n")
            f.write("-" * 20 + "\n")
            for key, value in data['metadata'].items():
                f.write(f"{key}: {value}\n")
            f.write("\n")
        
        # å¯«å…¥ç« ç¯€
        if 'sections' in data:
            f.write("ğŸ“š ç« ç¯€å…§å®¹\n")
            f.write("-" * 20 + "\n")
            for i, section in enumerate(data['sections'], 1):
                f.write(f"[ç« ç¯€ {i}] {section['title']}\n")
                f.write(f"{section['content']}\n\n")
        
        # å¯«å…¥æ®µè½
        if 'paragraphs' in data:
            f.write("ğŸ“„ æ®µè½å…§å®¹\n")
            f.write("-" * 20 + "\n")
            for i, para in enumerate(data['paragraphs'], 1):
                f.write(f"[æ®µè½ {i}]\n{para}\n\n")


def process_single_pdf(pdf_path):
    """è™•ç†å–®å€‹PDFæ–‡ä»¶"""
    print(f"ğŸ“„ æ­£åœ¨è™•ç†: {pdf_path.name}")
    
    # æå–åŸå§‹æ–‡æœ¬
    raw_text = extract_text_from_pdf(pdf_path)
    if not raw_text:
        print(f"âŒ ç„¡æ³•æå–æ–‡æœ¬: {pdf_path.name}")
        return
    
    # æ¸…ç†æ–‡æœ¬
    cleaned_text = clean_text(raw_text)
    
    # æå–è³‡æ–™
    metadata = extract_paper_metadata(cleaned_text)
    sections = split_into_sections(cleaned_text)
    paragraphs = split_into_paragraphs(cleaned_text)
    
    # çµ„ç¹”è³‡æ–™
    processed_data = {
        'source_file': pdf_path.name,
        'processed_time': datetime.now().isoformat(),
        'metadata': metadata,
        'sections': sections,
        'paragraphs': paragraphs,
        'statistics': {
            'total_text_length': len(cleaned_text),
            'section_count': len(sections),
            'paragraph_count': len(paragraphs)
        }
    }
    
    # ä½¿ç”¨æª”åçš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆä¾‹å¦‚ '2104' from '2104.05740v1.pdf'ï¼‰ä½œç‚ºæª”å
    filename_base = pdf_path.name.split('.')[0]
    output_file = OUTPUT_DIR / filename_base
    save_processed_data(processed_data, output_file)
    
    print(f"âœ… è™•ç†å®Œæˆ: {pdf_path.name}")
    print(f"   - ç« ç¯€æ•¸: {len(sections)}")
    print(f"   - æ®µè½æ•¸: {len(paragraphs)}")
    print(f"   - æ–‡æœ¬é•·åº¦: {len(cleaned_text):,} å­—å…ƒ")
    
    return processed_data


def process_all_pdfs():
    """è™•ç†æ‰€æœ‰PDFæ–‡ä»¶"""
    pdf_files = list(RAW_DIR.glob("*.pdf"))
    if not pdf_files:
        print("â— æ‰¾ä¸åˆ°ä»»ä½• PDF è«–æ–‡ï¼Œè«‹ç¢ºèª data/raw/ è³‡æ–™å¤¾")
        return
    
    print(f"ğŸš€ é–‹å§‹è™•ç† {len(pdf_files)} å€‹PDFæ–‡ä»¶...")
    
    results = []
    for pdf_path in pdf_files:
        try:
            result = process_single_pdf(pdf_path)
            if result:
                results.append(result)
        except Exception as e:
            print(f"âŒ è™•ç†å¤±æ•— {pdf_path.name}: {e}")
    
    print(f"\nğŸ“Š è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)} å€‹æ–‡ä»¶")
    
    # å„²å­˜è™•ç†æ‘˜è¦
    summary_path = OUTPUT_DIR / "processing_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump({
            'total_files': len(pdf_files),
            'successful_files': len(results),
            'processing_time': datetime.now().isoformat(),
            'file_statistics': [
                {
                    'filename': r['source_file'],
                    'sections': r['statistics']['section_count'],
                    'paragraphs': r['statistics']['paragraph_count'],
                    'text_length': r['statistics']['total_text_length']
                }
                for r in results
            ]
        }, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    process_all_pdfs()